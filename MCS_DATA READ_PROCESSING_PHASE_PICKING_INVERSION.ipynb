{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a3547-00f1-4014-bcdc-a08459582e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Install 'mplcursors' for interactive hover annotations on plots.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import signal, stats  # Added stats for kurtosis/skewness\n",
    "try:\n",
    "    import mplcursors\n",
    "    HAVE_MPLCURSORS = True\n",
    "except ImportError:\n",
    "    HAVE_MPLCURSORS = False\n",
    "    print(\"Note: Install 'mplcursors' for interactive hover annotations on plots.\")\n",
    "\n",
    "# ---------------------------\n",
    "# User Input: File paths and options\n",
    "# ---------------------------\n",
    "data_file_path = input(\"Enter path to seismic data file (.mcs or .segy): \").strip()\n",
    "while data_file_path == \"\":\n",
    "    data_file_path = input(\"Data file is required. Please enter the path to the data file: \").strip()\n",
    "\n",
    "log_file_path = input(\"Enter path to log file (.log) [press Enter if none]: \").strip()\n",
    "aux_file_path = input(\"Enter path to aux file (.aux) [press Enter if none]: \").strip()\n",
    "\n",
    "# Station metadata (for labeling plots); user can override if desired\n",
    "default_station = \"KNR24\"\n",
    "default_network = \"1B\"\n",
    "station_name = input(f\"Enter station name [default: {default_station}]: \").strip() or default_station\n",
    "network_code = input(f\"Enter network code [default: {default_network}]: \").strip() or default_network\n",
    "\n",
    "# Filtering and processing parameters\n",
    "# Bandpass filter range for analysis:\n",
    "bp_low = input(\"Enter low-cut frequency for bandpass filter [default 0.1 Hz]: \").strip()\n",
    "bp_high = input(\"Enter high-cut frequency for bandpass filter [default 20 Hz]: \").strip()\n",
    "lowcut_freq = float(bp_low) if bp_low else 0.1\n",
    "highcut_freq = float(bp_high) if bp_high else 20.0\n",
    "\n",
    "# Notch filter for powerline noise:\n",
    "notch = input(\"Enter powerline frequency for notch filter (0 to skip) [default 50 Hz]: \").strip()\n",
    "notch_freq = float(notch) if notch else 50.0\n",
    "if notch_freq == 0:\n",
    "    notch_freq = None  # Use None to indicate no notch filtering\n",
    "\n",
    "# Prompt for analysis sections\n",
    "adv_choice = input(\"Perform advanced signal analysis (PSD, correlations, etc)? [Y/n]: \").strip().lower()\n",
    "do_advanced = False if adv_choice == 'n' or adv_choice == 'no' else True\n",
    "\n",
    "clean_choice = input(\"Perform data cleaning and comparison analysis? [Y/n]: \").strip().lower()\n",
    "do_cleaning = False if clean_choice == 'n' or clean_choice == 'no' else True\n",
    "\n",
    "inv_choice = input(\"Perform travel-time inversion analysis? [Y/n]: \").strip().lower()\n",
    "do_inversion = False if inv_choice == 'n' or inv_choice == 'no' else True\n",
    "\n",
    "phase_choice = \"both\"\n",
    "if do_inversion:\n",
    "    phase_choice = input(\"Pick phases for inversion: 'P' for P-waves only, 'S' for S-waves only, 'Both' for both [default: Both]: \").strip().lower() or \"both\"\n",
    "    # Normalize input\n",
    "    if phase_choice not in ['p', 's', 'both']:\n",
    "        phase_choice = 'both'\n",
    "pick_p = (phase_choice in ['p', 'both'])\n",
    "pick_s = (phase_choice in ['s', 'both'])\n",
    "\n",
    "# Option to save plots to files\n",
    "save_plots_input = input(\"Save plots to files? [y/N]: \").strip().lower()\n",
    "save_plots = True if save_plots_input == 'y' or save_plots_input == 'yes' else False\n",
    "if save_plots:\n",
    "    output_dir = input(\"Enter directory to save plots [default: current directory]: \").strip() or \".\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Plots will be saved in: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load and parse log/aux files if provided\n",
    "# ---------------------------\n",
    "def parse_log_file(log_file_path):\n",
    "    \"\"\"Parse the .log file for recording information.\"\"\"\n",
    "    log_data = {\n",
    "        'start_time': None,\n",
    "        'end_time': None,\n",
    "        'sample_rate': None,\n",
    "        'file_size': None,\n",
    "        'checksum': None,\n",
    "        'errors': []\n",
    "    }\n",
    "    if not log_file_path or not os.path.exists(log_file_path):\n",
    "        return log_data\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'Start time:' in line:\n",
    "                try:\n",
    "                    time_str = line.split('Start time:')[1].strip()\n",
    "                    log_data['start_time'] = datetime.fromisoformat(time_str.replace(' ', 'T'))\n",
    "                except Exception:\n",
    "                    log_data['errors'].append(f\"Could not parse start time: {line}\")\n",
    "            elif 'End time:' in line:\n",
    "                try:\n",
    "                    time_str = line.split('End time:')[1].strip()\n",
    "                    log_data['end_time'] = datetime.fromisoformat(time_str.replace(' ', 'T'))\n",
    "                except Exception:\n",
    "                    log_data['errors'].append(f\"Could not parse end time: {line}\")\n",
    "            elif 'Sample rate:' in line:\n",
    "                try:\n",
    "                    rate_str = line.split('Sample rate:')[1].strip().split(' ')[0]\n",
    "                    log_data['sample_rate'] = float(rate_str)\n",
    "                except Exception:\n",
    "                    log_data['errors'].append(f\"Could not parse sample rate: {line}\")\n",
    "            elif 'File size:' in line:\n",
    "                try:\n",
    "                    size_str = line.split('File size:')[1].strip().split(' ')[0]\n",
    "                    log_data['file_size'] = int(size_str)\n",
    "                except Exception:\n",
    "                    log_data['errors'].append(f\"Could not parse file size: {line}\")\n",
    "            elif 'Checksum:' in line:\n",
    "                try:\n",
    "                    checksum_str = line.split('Checksum:')[1].strip()\n",
    "                    log_data['checksum'] = checksum_str\n",
    "                except Exception:\n",
    "                    log_data['errors'].append(f\"Could not parse checksum: {line}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading log file: {e}\")\n",
    "    return log_data\n",
    "\n",
    "def parse_aux_file(aux_file_path):\n",
    "    \"\"\"Parse the .aux file for auxiliary information.\"\"\"\n",
    "    aux_data = {\n",
    "        'gps_times': [],\n",
    "        'temperature_readings': [],\n",
    "        'battery_voltage': [],\n",
    "        'clock_drift': [],\n",
    "        'events': []\n",
    "    }\n",
    "    if not aux_file_path or not os.path.exists(aux_file_path):\n",
    "        return aux_data\n",
    "    try:\n",
    "        with open(aux_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('GPS:'):\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        time_str = parts[1] + 'T' + parts[2]\n",
    "                        aux_data['gps_times'].append(datetime.fromisoformat(time_str))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith('TEMP:'):\n",
    "                try:\n",
    "                    temp_str = line.split('TEMP:')[1].strip().split(' ')[0]\n",
    "                    aux_data['temperature_readings'].append(float(temp_str))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith('BATT:'):\n",
    "                try:\n",
    "                    batt_str = line.split('BATT:')[1].strip().split(' ')[0]\n",
    "                    aux_data['battery_voltage'].append(float(batt_str))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith('DRIFT:'):\n",
    "                try:\n",
    "                    drift_str = line.split('DRIFT:')[1].strip().split(' ')[0]\n",
    "                    aux_data['clock_drift'].append(float(drift_str))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif line.startswith('EVENT:'):\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        time_str = parts[1] + 'T' + parts[2]\n",
    "                        aux_data['events'].append(datetime.fromisoformat(time_str))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading aux file: {e}\")\n",
    "    return aux_data\n",
    "\n",
    "# Parse provided log/aux files\n",
    "log_data = parse_log_file(log_file_path)\n",
    "aux_data = parse_aux_file(aux_file_path)\n",
    "\n",
    "if log_data['start_time']:\n",
    "    print(f\"Log file start time: {log_data['start_time']}\")\n",
    "if log_data['end_time']:\n",
    "    print(f\"Log file end time: {log_data['end_time']}\")\n",
    "if log_data['sample_rate']:\n",
    "    print(f\"Log file sample rate: {log_data['sample_rate']} Hz\")\n",
    "if aux_data['gps_times']:\n",
    "    print(f\"Aux file GPS entries: {len(aux_data['gps_times'])}\")\n",
    "if aux_data['events']:\n",
    "    print(f\"Aux file events: {len(aux_data['events'])} detected events\")\n",
    "\n",
    "# ---------------------------\n",
    "# Read main seismic data file\n",
    "# ---------------------------\n",
    "# Determine file format and reading method\n",
    "file_ext = os.path.splitext(data_file_path)[1].lower()\n",
    "use_obspy = False\n",
    "if file_ext in ['.segy', '.sgy']:\n",
    "    try:\n",
    "        from obspy import read\n",
    "        use_obspy = True\n",
    "    except ImportError:\n",
    "        print(\"ObsPy not installed, cannot read SEGY directly. Please install obspy or convert the file.\")\n",
    "        use_obspy = False\n",
    "\n",
    "if use_obspy:\n",
    "    print(f\"Reading SEGY file: {data_file_path} ...\")\n",
    "    st = read(data_file_path)\n",
    "    # Merge to one trace per channel if needed\n",
    "    st.merge(fill_value=0)\n",
    "    # Assuming multiple channels (traces) in st correspond to different components\n",
    "    num_channels = len(st)\n",
    "    sample_rate = st[0].stats.sampling_rate\n",
    "    recording_start = st[0].stats.starttime.datetime  # Obspy UTCDateTime to datetime\n",
    "    data = np.array([tr.data for tr in st]).T.astype(np.float64)  # shape (n_samples, n_channels)\n",
    "    print(f\"Loaded {num_channels} channels from SEGY. Sample rate = {sample_rate} Hz.\")\n",
    "else:\n",
    "    # For .mcs or other binary format similar to the provided code\n",
    "    print(f\"Reading binary file: {data_file_path} ...\")\n",
    "    try:\n",
    "        file_size = os.path.getsize(data_file_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: File '{data_file_path}' not found.\")\n",
    "    # Use metadata sample rate if log did not provide\n",
    "    sample_rate = int(log_data['sample_rate'] or 100)  # default 100 if not in log\n",
    "    # The .mcs format specifics (24-bit, 4 channels, 4096-byte header) as in original code\n",
    "    num_channels = 4\n",
    "    bits_per_sample = 24\n",
    "    header_size_bytes = 4096\n",
    "    bytes_per_sample = bits_per_sample // 8  # 3 bytes\n",
    "    bytes_per_time_step = bytes_per_sample * num_channels\n",
    "\n",
    "    total_data_bytes = file_size - header_size_bytes\n",
    "    if total_data_bytes <= 0:\n",
    "        raise ValueError(\"File is too small or header size is larger than file size.\")\n",
    "    with open(data_file_path, 'rb') as f:\n",
    "        f.seek(header_size_bytes)\n",
    "        raw_bytes = f.read(total_data_bytes)\n",
    "    raw_data = np.frombuffer(raw_bytes, dtype=np.uint8)\n",
    "    # Ensure we have whole time steps\n",
    "    total_bytes = len(raw_data)\n",
    "    complete_time_steps = total_bytes // bytes_per_time_step\n",
    "    if complete_time_steps == 0:\n",
    "        raise ValueError(\"No data samples found in file.\")\n",
    "    usable_bytes = complete_time_steps * bytes_per_time_step\n",
    "    if usable_bytes < total_bytes:\n",
    "        raw_data = raw_data[:usable_bytes]\n",
    "        print(f\"Warning: {total_bytes - usable_bytes} trailing bytes ignored (incomplete sample).\")\n",
    "\n",
    "    # Reshape raw data to [time_steps x bytes_per_time_step]\n",
    "    raw_data_reshaped = raw_data.reshape(-1, bytes_per_time_step)\n",
    "    # Prepare output array\n",
    "    data = np.zeros((raw_data_reshaped.shape[0], num_channels), dtype=np.float64)\n",
    "    # Gains: if available from metadata or default to 1\n",
    "    hydro_gain = 1.0\n",
    "    seis_gain = 1.0\n",
    "    # If metadata dictionary had these, use them\n",
    "    if log_data['sample_rate']:\n",
    "        sample_rate = int(log_data['sample_rate'])\n",
    "    # If known gain values in log or metadata can be used, else assume 1\n",
    "    # (Original metadata in code snippet hard-coded 4 and 1; we use 1 if unknown)\n",
    "    gain_factors = np.array([hydro_gain, seis_gain, seis_gain, seis_gain])\n",
    "\n",
    "    # Convert 3-byte values to int and apply gains\n",
    "    for ch in range(num_channels):\n",
    "        # indices in each frame for this channel's 3 bytes\n",
    "        idx = ch * bytes_per_sample\n",
    "        byte1 = raw_data_reshaped[:, idx]      # MSB\n",
    "        byte2 = raw_data_reshaped[:, idx + 1]  # mid byte\n",
    "        byte3 = raw_data_reshaped[:, idx + 2]  # LSB\n",
    "        # combine bytes into 24-bit signed int\n",
    "        samples = (byte1.astype(np.int32) << 16) | (byte2.astype(np.int32) << 8) | byte3.astype(np.int32)\n",
    "        # two's complement adjustment for 24-bit\n",
    "        samples = np.where(samples > 0x7FFFFF, samples - 0x1000000, samples)\n",
    "        # apply gain\n",
    "        data[:, ch] = samples.astype(np.float64) / gain_factors[ch]\n",
    "\n",
    "    recording_start = log_data['start_time'] or datetime.now()\n",
    "    print(f\"Binary file read complete. Samples: {data.shape[0]}, Channels: {num_channels}, Sample rate: {sample_rate} Hz.\")\n",
    "\n",
    "# Channel naming based on assumptions or metadata (adjust if needed)\n",
    "channel_names = []\n",
    "channel_units = []\n",
    "if num_channels == 4:\n",
    "    channel_names = ['Hydrophone', 'Seismometer Z', 'Seismometer Y', 'Seismometer X']\n",
    "    channel_units = ['Pressure [Pa]', 'Amplitude', 'Amplitude', 'Amplitude']\n",
    "else:\n",
    "    # For unknown channel count, label generically\n",
    "    channel_names = [f\"Channel {i+1}\" for i in range(num_channels)]\n",
    "    channel_units = ['Amplitude'] * num_channels\n",
    "\n",
    "# ---------------------------\n",
    "# Signal amplitude detection and basic analysis\n",
    "# ---------------------------\n",
    "# Define helper to check if a signal chunk has significant amplitude (not just noise)\n",
    "def has_amplitude(signal_data, noise_threshold=0.01, flat_threshold=1e-6):\n",
    "    if len(signal_data) == 0:\n",
    "        return False\n",
    "    std_dev = np.std(signal_data)\n",
    "    data_range = np.ptp(signal_data)\n",
    "    max_abs = np.max(np.abs(signal_data))\n",
    "    return (std_dev > noise_threshold and data_range > flat_threshold and max_abs > noise_threshold * 2)\n",
    "\n",
    "# Find end of data where no significant amplitude is detected (assume after event)\n",
    "def find_amplitude_end(data, sample_rate, chunk_size_seconds=10, noise_threshold=0.01):\n",
    "    chunk_size = int(chunk_size_seconds * sample_rate)\n",
    "    total_samples = data.shape[0]\n",
    "    for start in range(0, total_samples, chunk_size):\n",
    "        end = min(start + chunk_size, total_samples)\n",
    "        chunk = data[start:end, :]\n",
    "        # if no channel has amplitude in this chunk, consider this the end\n",
    "        if not any(has_amplitude(chunk[:, ch], noise_threshold) for ch in range(data.shape[1])):\n",
    "            return start\n",
    "    return total_samples\n",
    "\n",
    "# Determine region with signal (assuming after that is just noise)\n",
    "amplitude_end_idx = find_amplitude_end(data, sample_rate)\n",
    "if amplitude_end_idx < data.shape[0]:\n",
    "    print(f\"No significant signal after sample {amplitude_end_idx} (approx {amplitude_end_idx/sample_rate:.1f}s). Truncating data to signal region.\")\n",
    "    data_with_amplitude = data[:amplitude_end_idx, :]\n",
    "else:\n",
    "    data_with_amplitude = data  # no quiet part detected, use all data\n",
    "total_samples = data_with_amplitude.shape[0]\n",
    "total_duration_sec = total_samples / sample_rate\n",
    "recording_end_amplitude = recording_start + timedelta(seconds=total_duration_sec)\n",
    "print(f\"Using {total_samples} samples (~{total_duration_sec:.2f} seconds) containing the signal/event.\")\n",
    "\n",
    "# Compute amplitude envelope for each channel (to analyze signal strength over time)\n",
    "def calculate_amplitude_envelope(signal_data, fs, smooth_window=1.0):\n",
    "    analytic = signal.hilbert(signal_data)\n",
    "    envelope = np.abs(analytic)\n",
    "    # Smooth envelope with moving average of specified window (seconds)\n",
    "    window_pts = int(smooth_window * fs)\n",
    "    if window_pts > 1:\n",
    "        envelope = uniform_filter1d(envelope, size=window_pts)\n",
    "    return envelope\n",
    "\n",
    "envelopes = np.zeros_like(data_with_amplitude)\n",
    "for i in range(num_channels):\n",
    "    envelopes[:, i] = calculate_amplitude_envelope(data_with_amplitude[:, i], sample_rate, window_size=1.0)\n",
    "\n",
    "# Plot overall signal strength (envelope) over time for each channel (log scale)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_channels):\n",
    "    if i == 0:\n",
    "        plt.semilogy(np.arange(total_samples)/sample_rate, np.abs(data_with_amplitude[:, i]), \n",
    "                     label=f\"{channel_names[i]} (Pressure)\", alpha=0.7)\n",
    "    else:\n",
    "        plt.semilogy(np.arange(total_samples)/sample_rate, envelopes[:, i], \n",
    "                     label=f\"{channel_names[i]} (Amplitude)\", alpha=0.7)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Signal Strength (log scale)')\n",
    "plt.title(f'Signal Strength Over Time - Station {station_name}')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "# Save and show plot\n",
    "if save_plots:\n",
    "    fname = f\"{station_name}_{network_code}_signal_strength.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot: {fname}\")\n",
    "print(\"Displaying signal strength plot... Close the figure window to continue.\")\n",
    "if HAVE_MPLCURSORS:\n",
    "    mplcursors.cursor(hover=True)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Spectrogram for each channel (if signal present)\n",
    "# ---------------------------\n",
    "print(\"Generating spectrograms for each channel...\")\n",
    "fig, axes = plt.subplots( (num_channels+1)//2, 2, figsize=(12, 6*((num_channels+1)//2)) )\n",
    "axes = axes.flatten() if num_channels > 1 else [axes]\n",
    "for i in range(num_channels):\n",
    "    ax = axes[i]\n",
    "    if has_amplitude(data_with_amplitude[:, i]):\n",
    "        f, t, Sxx = signal.spectrogram(data_with_amplitude[:, i], fs=sample_rate, nperseg=256, noverlap=128, nfft=512, scaling='density')\n",
    "        Sxx_db = 10 * np.log10(Sxx + 1e-10)\n",
    "        im = ax.pcolormesh(t, f, Sxx_db, shading='gouraud', cmap='viridis')\n",
    "        ax.set_ylim(0, min(50, sample_rate/2))\n",
    "        ax.set_xlabel('Time [s]')\n",
    "        ax.set_ylabel('Frequency [Hz]')\n",
    "        ax.set_title(f\"{channel_names[i]} Spectrogram\")\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('PSD [dB]')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No signal', transform=ax.transAxes, ha='center', va='center', fontsize=12, color='gray')\n",
    "        ax.set_title(f\"{channel_names[i]} Spectrogram\")\n",
    "        ax.set_xlabel('Time [s]')\n",
    "        ax.set_ylabel('Frequency [Hz]')\n",
    "# Hide any unused subplot axes\n",
    "for j in range(num_channels, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "plt.suptitle(f\"Spectrograms - Station {station_name}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "# Save and show spectrograms\n",
    "if save_plots:\n",
    "    fname = f\"{station_name}_{network_code}_spectrograms.png\"\n",
    "    plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot: {fname}\")\n",
    "print(\"Displaying spectrograms... Close the figure window to continue.\")\n",
    "# (Hover cursor not as useful on spectrogram heatmap, so we skip mplcursors here to avoid clutter)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Optional: Plot segmented waveform (e.g., first N segments of data)\n",
    "# ---------------------------\n",
    "try:\n",
    "    seg_count = int(input(\"Enter number of 5-second segments to plot from signal (0 to skip) [default 0]: \") or 0)\n",
    "except:\n",
    "    seg_count = 0\n",
    "segment_duration = 5  # seconds per segment\n",
    "samples_per_segment = int(segment_duration * sample_rate)\n",
    "if seg_count > 0:\n",
    "    max_segments = data_with_amplitude.shape[0] // samples_per_segment\n",
    "    seg_count = min(seg_count, max_segments)\n",
    "    for segment in range(seg_count):\n",
    "        start_idx = segment * samples_per_segment\n",
    "        end_idx = start_idx + samples_per_segment\n",
    "        seg_data = data_with_amplitude[start_idx:end_idx, :]\n",
    "        seg_time = np.arange(start_idx, end_idx) / sample_rate\n",
    "        fig, axes = plt.subplots(num_channels, 1, figsize=(12, 8), sharex=True)\n",
    "        for i in range(num_channels):\n",
    "            ax = axes[i]\n",
    "            if has_amplitude(seg_data[:, i]):\n",
    "                ax.plot(seg_time, seg_data[:, i], color='C0', linewidth=0.8)\n",
    "                # Mark events (from aux file) if they fall in this segment\n",
    "                for ev_time in aux_data.get('events', []):\n",
    "                    ev_sec = (ev_time - recording_start).total_seconds()\n",
    "                    if ev_sec >= seg_time[0] and ev_sec <= seg_time[-1]:\n",
    "                        ax.axvline(x=ev_sec, color='red', ls='--', lw=0.8)\n",
    "                        ax.text(ev_sec, 0.9*ax.get_ylim()[1], 'EVENT', color='red', rotation=90, va='top', fontsize=8)\n",
    "                ax.set_ylabel(channel_units[i])\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No signal', transform=ax.transAxes, ha='center', va='center', fontsize=10, color='gray')\n",
    "            ax.set_title(f\"{channel_names[i]}\")\n",
    "            ax.grid(True, ls='--', alpha=0.5)\n",
    "        axes[-1].set_xlabel(\"Time [s]\")\n",
    "        plt.suptitle(f\"{station_name} Segment {segment+1}: {segment*segment_duration}-{(segment+1)*segment_duration}s\")\n",
    "        plt.tight_layout()\n",
    "        # Save and show\n",
    "        if save_plots:\n",
    "            fname = f\"{station_name}_{network_code}_segment{segment+1}_{segment*segment_duration}-{(segment+1)*segment_duration}s.png\"\n",
    "            plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved plot: {fname}\")\n",
    "        print(f\"Displaying segment {segment+1}/{seg_count}... Close the figure to continue.\")\n",
    "        if HAVE_MPLCURSORS:\n",
    "            mplcursors.cursor(hover=True)\n",
    "        plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Advanced data analysis (optional)\n",
    "# ---------------------------\n",
    "if do_advanced:\n",
    "    print(\"\\n=== Advanced Data Analysis ===\")\n",
    "    # Define advanced processing functions (bandpass, PSD, correlations, etc.)\n",
    "    def apply_bandpass_filter(data, fs, lowcut=1.0, highcut=20.0, order=4):\n",
    "        nyquist = 0.5 * fs\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = signal.butter(order, [low, high], btype='band')\n",
    "        return signal.filtfilt(b, a, data)\n",
    "    def calculate_psd_vals(data, fs, nperseg=None):\n",
    "        if nperseg is None:\n",
    "            nperseg = min(256, len(data)//2)\n",
    "            if nperseg < 8:\n",
    "                nperseg = len(data)\n",
    "        f, Pxx = signal.welch(data, fs=fs, nperseg=nperseg, scaling='density')\n",
    "        return f, Pxx\n",
    "    def calculate_cross_corr(x, y, max_lag_samples=1000):\n",
    "        corr = signal.correlate(x, y, mode='full')\n",
    "        lags = signal.correlation_lags(len(x), len(y), mode='full')\n",
    "        mask = np.abs(lags) <= max_lag_samples\n",
    "        return lags[mask], corr[mask]\n",
    "    def calculate_auto_corr(x, max_lag_samples=500):\n",
    "        corr = signal.correlate(x, x, mode='full')\n",
    "        lags = signal.correlation_lags(len(x), len(x), mode='full')\n",
    "        corr = corr / np.max(corr)  # normalize\n",
    "        mask = np.abs(lags) <= max_lag_samples\n",
    "        return lags[mask], corr[mask]\n",
    "    def detect_events_sta_lta(data, fs, threshold=5.0, min_distance=1000):\n",
    "        short_win = int(1 * fs)\n",
    "        long_win = int(10 * fs)\n",
    "        sta = uniform_filter1d(np.abs(data), size=short_win)\n",
    "        lta = uniform_filter1d(np.abs(data), size=long_win)\n",
    "        ratio = sta / (lta + 1e-10)\n",
    "        peaks, _ = signal.find_peaks(ratio, height=threshold, distance=min_distance)\n",
    "        return peaks, ratio\n",
    "    def analyze_signal_quality(data, fs):\n",
    "        metrics = {}\n",
    "        # SNR: ratio of total variance to variance of high-frequency residual as noise\n",
    "        smooth = uniform_filter1d(data, size=min(100, len(data)//10))\n",
    "        noise = data - smooth\n",
    "        signal_power = np.var(data)\n",
    "        noise_power = np.var(noise)\n",
    "        metrics['snr_db'] = 10 * np.log10(signal_power / (noise_power + 1e-10))\n",
    "        # Dynamic range\n",
    "        metrics['dynamic_range_db'] = 20 * np.log10(np.max(np.abs(data)) / (np.std(data) + 1e-10))\n",
    "        # Zero-crossing rate\n",
    "        zero_crossings = np.where(np.diff(np.sign(data)))[0]\n",
    "        metrics['zero_crossing_rate'] = len(zero_crossings) / (len(data) / fs)\n",
    "        return metrics\n",
    "    def perform_polarization_analysis(z, ns, ew, fs, window_size=1000):\n",
    "        results = []\n",
    "        for i in range(0, len(z) - window_size, window_size // 2):\n",
    "            win_z = z[i:i+window_size]\n",
    "            win_ns = ns[i:i+window_size]\n",
    "            win_ew = ew[i:i+window_size]\n",
    "            cov = np.cov([win_z, win_ns, win_ew])\n",
    "            eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "            # sort eigenvalues\n",
    "            eigenvals = np.sort(eigenvals)\n",
    "            # compute polarization metrics\n",
    "            rect = 1 - (eigenvals[0] + eigenvals[1]) / (2*eigenvals[2] + 1e-10)\n",
    "            plan = 1 - (2*eigenvals[0]) / (eigenvals[1] + eigenvals[2] + 1e-10)\n",
    "            results.append({'time': i/fs, 'rectilinearity': rect, 'planarity': plan})\n",
    "        return results\n",
    "\n",
    "    # Apply bandpass filter (using user-provided lowcut/highcut or defaults)\n",
    "    filtered_data = np.zeros_like(data_with_amplitude)\n",
    "    for i in range(num_channels):\n",
    "        filtered_data[:, i] = apply_bandpass_filter(data_with_amplitude[:, i], sample_rate, lowcut=lowcut_freq, highcut=highcut_freq)\n",
    "    print(f\"Applied bandpass filter ({lowcut_freq}-{highcut_freq} Hz) to all channels.\")\n",
    "\n",
    "    # Power Spectral Density for each channel\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_channels):\n",
    "        f, Pxx = calculate_psd_vals(filtered_data[:, i], sample_rate)\n",
    "        plt.semilogy(f, Pxx, label=channel_names[i], alpha=0.8)\n",
    "    plt.title('Power Spectral Density (bandpass filtered data)')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('PSD [arb. units^2/Hz]')\n",
    "    plt.xlim(0, highcut_freq*1.5)\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_PSD.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying PSD plot... Close the figure to continue.\")\n",
    "    if HAVE_MPLCURSORS:\n",
    "        mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Cross-correlation between seismometer components (if 3 components exist)\n",
    "    if num_channels >= 3:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        axes = axes.flatten()\n",
    "        pairs = [(2, 3), (1, 3), (1, 2)]  # Y-X, Z-X, Z-Y if using names above\n",
    "        titles = ['Y-X Corr', 'Z-X Corr', 'Z-Y Corr']\n",
    "        for idx, (ch1, ch2) in enumerate(pairs):\n",
    "            if ch1 < num_channels and ch2 < num_channels:\n",
    "                lags, corr = calculate_cross_corr(filtered_data[:, ch1], filtered_data[:, ch2], max_lag_samples=500)\n",
    "                corr_norm = corr / (np.max(np.abs(corr)) + 1e-10)\n",
    "                axes[idx].plot(lags/sample_rate, corr_norm, 'k-')\n",
    "                axes[idx].set_title(titles[idx])\n",
    "                axes[idx].set_xlabel('Lag [s]')\n",
    "                axes[idx].set_ylabel('Correlation (norm)')\n",
    "                axes[idx].grid(True, ls='--', alpha=0.5)\n",
    "        axes[-1].axis('off')\n",
    "        plt.suptitle('Cross-correlation between components')\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            fname = f\"{station_name}_{network_code}_cross_correlation.png\"\n",
    "            plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved plot: {fname}\")\n",
    "        print(\"Displaying cross-correlation plots... Close the figure to continue.\")\n",
    "        plt.show()\n",
    "\n",
    "    # Auto-correlation for first up to 4 channels\n",
    "    fig, axes = plt.subplots(min(4, num_channels)//2 + 1, 2, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(min(num_channels, 4)):\n",
    "        lags, acorr = calculate_auto_corr(filtered_data[:, i], max_lag_samples=200)\n",
    "        axes[i].plot(lags/sample_rate, acorr, 'b-')\n",
    "        axes[i].set_title(f'Autocorrelation - {channel_names[i]}')\n",
    "        axes[i].set_xlabel('Lag [s]')\n",
    "        axes[i].set_ylabel('Correlation (norm)')\n",
    "        axes[i].grid(True, ls='--', alpha=0.5)\n",
    "    # Hide unused axes\n",
    "    for j in range(min(4, num_channels), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_autocorr.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying autocorrelation plots... Close figure to continue.\")\n",
    "    plt.show()\n",
    "\n",
    "    # Event detection (STA/LTA) on each seismometer channel\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_channels):\n",
    "        if channel_names[i].lower().startswith('seismometer') or num_channels <= 3:\n",
    "            peaks, ratio = detect_events_sta_lta(filtered_data[:, i], sample_rate, threshold=3.0, min_distance=int(1*sample_rate))\n",
    "            plt.plot(np.arange(len(ratio))/sample_rate, ratio, label=f\"{channel_names[i]}\")\n",
    "            plt.plot(np.array(peaks)/sample_rate, ratio[peaks], 'ro', markersize=4)\n",
    "            print(f\"{channel_names[i]}: {len(peaks)} event(s) detected by STA/LTA.\")\n",
    "    plt.title('Event Detection (STA/LTA)')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('STA/LTA Ratio')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, ls='--', alpha=0.5)\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_STA_LTA.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying STA/LTA event detection plot... Close figure to continue.\")\n",
    "    if HAVE_MPLCURSORS:\n",
    "        mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Signal quality metrics\n",
    "    for i in range(num_channels):\n",
    "        metrics = analyze_signal_quality(filtered_data[:, i], sample_rate)\n",
    "        print(f\"{channel_names[i]} - SNR: {metrics['snr_db']:.2f} dB, Dynamic Range: {metrics['dynamic_range_db']:.2f} dB, Zero-crossing rate: {metrics['zero_crossing_rate']:.2f} Hz\")\n",
    "\n",
    "    # Polarization analysis (if 3 components available)\n",
    "    if num_channels >= 4:\n",
    "        results = perform_polarization_analysis(filtered_data[:, 1], filtered_data[:, 2], filtered_data[:, 3], sample_rate)\n",
    "        times = [r['time'] for r in results]\n",
    "        rect_vals = [r['rectilinearity'] for r in results]\n",
    "        plan_vals = [r['planarity'] for r in results]\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "        axes[0].plot(times, rect_vals, 'b-')\n",
    "        axes[0].set_ylabel('Rectilinearity')\n",
    "        axes[0].set_title('Polarization Analysis')\n",
    "        axes[0].grid(True, ls='--', alpha=0.5)\n",
    "        axes[1].plot(times, plan_vals, 'r-')\n",
    "        axes[1].set_xlabel('Time [s]')\n",
    "        axes[1].set_ylabel('Planarity')\n",
    "        axes[1].grid(True, ls='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            fname = f\"{station_name}_{network_code}_polarization.png\"\n",
    "            plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved plot: {fname}\")\n",
    "        print(\"Displaying polarization analysis... Close figure to continue.\")\n",
    "        if HAVE_MPLCURSORS:\n",
    "            mplcursors.cursor(hover=True)\n",
    "        plt.show()\n",
    "\n",
    "    # Detailed spectrogram with log frequency scale\n",
    "    fig, axes = plt.subplots( (num_channels+1)//2, 2, figsize=(12, 6*((num_channels+1)//2)) )\n",
    "    axes = axes.flatten()\n",
    "    freq_bands = {'ULF': (0.001, 0.01), 'VLF': (0.01, 0.1), 'LF': (0.1, 1), 'MF': (1, 10), 'HF': (10, 50)}\n",
    "    for i in range(num_channels):\n",
    "        ax = axes[i]\n",
    "        f, t, Sxx = signal.spectrogram(filtered_data[:, i], fs=sample_rate, nperseg=min(512, len(filtered_data)//10), noverlap=256, nfft=min(1024, len(filtered_data)//5))\n",
    "        Sxx_db = 10 * np.log10(Sxx + 1e-10)\n",
    "        im = ax.pcolormesh(t, f, Sxx_db, shading='gouraud', cmap='viridis')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim(0.1, 50)\n",
    "        ax.set_xlabel('Time [s]')\n",
    "        ax.set_ylabel('Freq [Hz]')\n",
    "        ax.set_title(f'{channel_names[i]} Spectrogram (log f)')\n",
    "        # Mark frequency bands\n",
    "        for band, (f_low, f_high) in freq_bands.items():\n",
    "            if f_low >= ax.get_ylim()[0] and f_high <= ax.get_ylim()[1]:\n",
    "                ax.axhspan(f_low, f_high, color='gray', alpha=0.1)\n",
    "                ax.text(t.max()*0.95, np.sqrt(f_low*f_high), band, va='center', ha='right', fontsize=8, backgroundcolor='white')\n",
    "        plt.colorbar(im, ax=ax, label='PSD [dB]')\n",
    "    for j in range(num_channels, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.suptitle('Detailed Spectrograms')\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_detailed_spectrogram.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying detailed spectrograms... Close figure to continue.\")\n",
    "    plt.show()\n",
    "\n",
    "    # Statistical analysis of signals\n",
    "    for i in range(num_channels):\n",
    "        x = filtered_data[:, i]\n",
    "        stats_dict = {\n",
    "            'mean': np.mean(x), \n",
    "            'std': np.std(x), \n",
    "            'max': np.max(x), \n",
    "            'min': np.min(x), \n",
    "            'rms': np.sqrt(np.mean(x**2)), \n",
    "            'crest_factor': np.max(np.abs(x)) / (np.sqrt(np.mean(x**2)) + 1e-10),\n",
    "            'kurtosis': stats.kurtosis(x, fisher=False), \n",
    "            'skewness': stats.skew(x)\n",
    "        }\n",
    "        print(f\"\\n{channel_names[i]} stats:\")\n",
    "        for k, v in stats_dict.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "    # Comparative analysis (distribution & energy over time)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # Amplitude distribution\n",
    "    for i in range(num_channels):\n",
    "        axes[0].hist(filtered_data[:, i], bins=50, alpha=0.5, density=True, label=channel_names[i])\n",
    "    axes[0].set_title('Amplitude Distribution')\n",
    "    axes[0].set_xlabel('Amplitude')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, ls='--', alpha=0.5)\n",
    "    # Cumulative energy\n",
    "    time_axis = np.arange(filtered_data.shape[0]) / sample_rate\n",
    "    for i in range(num_channels):\n",
    "        energy = np.cumsum(filtered_data[:, i]**2)\n",
    "        axes[1].plot(time_axis, energy/energy[-1], label=channel_names[i])\n",
    "    axes[1].set_title('Cumulative Energy')\n",
    "    axes[1].set_xlabel('Time [s]')\n",
    "    axes[1].set_ylabel('Normalized Cumulative Energy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, ls='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_distribution_energy.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying distribution and energy plots... Close figure to continue.\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Data cleaning and comparison (optional)\n",
    "# ---------------------------\n",
    "if do_cleaning:\n",
    "    print(\"\\n=== Data Cleaning Pipeline ===\")\n",
    "else:\n",
    "    print(\"\\nSkipping data cleaning. Using raw data for phase picking/inversion.\")\n",
    "# Define cleaning functions\n",
    "def remove_instrument_response(data, fs, sensitivity=1.0):\n",
    "    nyquist = 0.5 * fs\n",
    "    highpass_freq = 0.01  # remove DC and very low freq\n",
    "    b, a = signal.butter(2, highpass_freq/nyquist, btype='high')\n",
    "    return signal.filtfilt(b, a, data) * sensitivity\n",
    "def remove_trend(data):\n",
    "    return signal.detrend(data, type='linear')\n",
    "def remove_mean(data):\n",
    "    return data - np.mean(data)\n",
    "def apply_taper(data, taper_percent=0.05):\n",
    "    n = len(data)\n",
    "    taper_len = int(n * taper_percent)\n",
    "    if taper_len < 1:\n",
    "        return data\n",
    "    taper = np.ones(n)\n",
    "    # cosine taper\n",
    "    t = np.arange(taper_len)\n",
    "    taper_window = 0.5 * (1 - np.cos(np.pi * t / taper_len))\n",
    "    taper[:taper_len] = taper_window\n",
    "    taper[-taper_len:] = taper_window[::-1]\n",
    "    return data * taper\n",
    "def remove_outliers_iqr(data, multiplier=1.5):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    low = Q1 - multiplier * IQR\n",
    "    high = Q3 + multiplier * IQR\n",
    "    cleaned = np.copy(data)\n",
    "    cleaned[cleaned < low] = low\n",
    "    cleaned[cleaned > high] = high\n",
    "    return cleaned\n",
    "def remove_glitches(data, threshold=5.0, window_size=10):\n",
    "    cleaned = np.copy(data)\n",
    "    std = np.std(data)\n",
    "    med = np.median(data)\n",
    "    diff = np.abs(data - med)\n",
    "    glitch_indices = np.where(diff > threshold * std)[0]\n",
    "    for i in glitch_indices:\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(data), i + window_size + 1)\n",
    "        cleaned[i] = np.median(data[start:end])\n",
    "    return cleaned\n",
    "def apply_notch_filter(data, fs, notch_freq=50.0, quality_factor=30.0):\n",
    "    if not notch_freq or notch_freq >= 0.5*fs:\n",
    "        return data\n",
    "    try:\n",
    "        # Newer SciPy usage with fs\n",
    "        b, a = signal.iirnotch(notch_freq, quality_factor, fs=fs)\n",
    "    except Exception:\n",
    "        # Fallback if fs param not supported\n",
    "        w0 = notch_freq / (0.5*fs)\n",
    "        if w0 <= 0 or w0 >= 1:\n",
    "            return data\n",
    "        b, a = signal.iirnotch(w0, quality_factor)\n",
    "    return signal.filtfilt(b, a, data)\n",
    "def apply_bandpass_filter_clean(data, fs, lowcut=0.1, highcut=20.0, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    if lowcut >= nyquist or highcut >= nyquist:\n",
    "        return data\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    return signal.filtfilt(b, a, data)\n",
    "def apply_adaptive_filter(data, noise_window=1000):\n",
    "    # simple noise removal by subtracting running average\n",
    "    noise_est = uniform_filter1d(data, size=noise_window)\n",
    "    return data - noise_est\n",
    "\n",
    "# Apply cleaning steps channel by channel\n",
    "if do_cleaning:\n",
    "    cleaned_data = np.zeros_like(data_with_amplitude)\n",
    "    cleaning_steps = []\n",
    "    for i in range(num_channels):\n",
    "        ch_data = data_with_amplitude[:, i].copy()\n",
    "        # Step 1: remove DC offset\n",
    "        ch_data = remove_mean(ch_data); step1 = \"Removed DC offset\"\n",
    "        # Step 2: remove linear trend\n",
    "        ch_data = remove_trend(ch_data); step2 = \"Removed linear trend\"\n",
    "        # Step 3: taper edges\n",
    "        ch_data = apply_taper(ch_data); step3 = \"Tapered edges\"\n",
    "        # Step 4: remove outliers\n",
    "        ch_data = remove_outliers_iqr(ch_data); step4 = \"Removed outliers (IQR)\"\n",
    "        # Step 5: remove glitches\n",
    "        ch_data = remove_glitches(ch_data); step5 = \"Removed short glitches\"\n",
    "        # Step 6: remove instrument response (simplified highpass)\n",
    "        ch_data = remove_instrument_response(ch_data, sample_rate); step6 = \"Removed instrument response\"\n",
    "        # Step 7: notch filter (power line)\n",
    "        if notch_freq:\n",
    "            ch_data = apply_notch_filter(ch_data, sample_rate, notch_freq=notch_freq); step7 = f\"Notch filter ({notch_freq}Hz)\"\n",
    "        else:\n",
    "            step7 = \"No notch filter\"\n",
    "        # Step 8: bandpass filter\n",
    "        ch_data = apply_bandpass_filter_clean(ch_data, sample_rate, lowcut=lowcut_freq, highcut=highcut_freq); step8 = f\"Bandpass {lowcut_freq}-{highcut_freq}Hz\"\n",
    "        # Step 9: adaptive noise cancel\n",
    "        ch_data = apply_adaptive_filter(ch_data, noise_window=1000); step9 = \"Adaptive noise cancellation\"\n",
    "        cleaned_data[:, i] = ch_data\n",
    "        steps = [step1, step2, step3, step4, step5, step6, step7, step8, step9]\n",
    "        cleaning_steps.append((channel_names[i], steps))\n",
    "        print(f\"{channel_names[i]} cleaned with steps: \" + \", \".join(steps))\n",
    "else:\n",
    "    # If not cleaning, just set cleaned_data to original truncated data for downstream compatibility\n",
    "    cleaned_data = data_with_amplitude\n",
    "\n",
    "# Compare original vs cleaned (if cleaning was done)\n",
    "if do_cleaning:\n",
    "    # Time domain comparison\n",
    "    fig, axes = plt.subplots(num_channels, 2, figsize=(12, 3*num_channels))\n",
    "    for i in range(num_channels):\n",
    "        axes[i, 0].plot(np.arange(data_with_amplitude.shape[0])/sample_rate, data_with_amplitude[:, i], 'gray', label='Original', linewidth=0.8)\n",
    "        axes[i, 0].set_ylabel(channel_units[i])\n",
    "        axes[i, 0].set_title(f\"{channel_names[i]} - Original\")\n",
    "        axes[i, 0].grid(True, ls='--', alpha=0.5)\n",
    "        axes[i, 1].plot(np.arange(cleaned_data.shape[0])/sample_rate, cleaned_data[:, i], 'b', label='Cleaned', linewidth=0.8)\n",
    "        axes[i, 1].set_title(f\"{channel_names[i]} - Cleaned\")\n",
    "        axes[i, 1].grid(True, ls='--', alpha=0.5)\n",
    "        if i == num_channels-1:\n",
    "            axes[i, 0].set_xlabel('Time [s]')\n",
    "            axes[i, 1].set_xlabel('Time [s]')\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_time_compare.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying time domain comparison (original vs cleaned)... Close figure to continue.\")\n",
    "    if HAVE_MPLCURSORS:\n",
    "        mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Frequency domain (PSD) comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_channels):\n",
    "        f_orig, Pxx_orig = signal.welch(data_with_amplitude[:, i], fs=sample_rate, nperseg=256)\n",
    "        f_clean, Pxx_clean = signal.welch(cleaned_data[:, i], fs=sample_rate, nperseg=256)\n",
    "        plt.semilogy(f_orig, Pxx_orig, 'gray', alpha=0.5)\n",
    "        plt.semilogy(f_clean, Pxx_clean, label=channel_names[i])\n",
    "    plt.title('PSD Comparison (Original vs Cleaned)')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('Power Spectral Density')\n",
    "    plt.xlim(0, highcut_freq*1.5)\n",
    "    plt.grid(True, which='both', ls='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_PSD_compare.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying PSD comparison... Close figure to continue.\")\n",
    "    plt.show()\n",
    "\n",
    "    # Spectrogram comparison for one channel (e.g., vertical seismometer if exists)\n",
    "    ch_to_show = 1 if num_channels > 1 else 0\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    f, t, Sxx_orig = signal.spectrogram(data_with_amplitude[:, ch_to_show], fs=sample_rate, nperseg=256, noverlap=128)\n",
    "    f, t, Sxx_clean = signal.spectrogram(cleaned_data[:, ch_to_show], fs=sample_rate, nperseg=256, noverlap=128)\n",
    "    axes[0].pcolormesh(t, f, 10*np.log10(Sxx_orig+1e-10), shading='gouraud', cmap='viridis')\n",
    "    axes[0].set_title(f\"Original {channel_names[ch_to_show]} Spectrogram\")\n",
    "    axes[0].set_ylim(0, min(50, sample_rate/2))\n",
    "    axes[0].set_ylabel('Frequency [Hz]')\n",
    "    axes[0].set_xlabel('Time [s]')\n",
    "    axes[1].pcolormesh(t, f, 10*np.log10(Sxx_clean+1e-10), shading='gouraud', cmap='viridis')\n",
    "    axes[1].set_title(f\"Cleaned {channel_names[ch_to_show]} Spectrogram\")\n",
    "    axes[1].set_ylim(0, min(50, sample_rate/2))\n",
    "    axes[1].set_xlabel('Time [s]')\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_spectrogram_compare.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying spectrogram comparison... Close figure to continue.\")\n",
    "    plt.show()\n",
    "\n",
    "    # Histograms comparison\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_channels):\n",
    "        plt.hist(data_with_amplitude[:, i], bins=50, alpha=0.5, label=f\"{channel_names[i]} Original\", density=True)\n",
    "        plt.hist(cleaned_data[:, i], bins=50, alpha=0.5, label=f\"{channel_names[i]} Cleaned\", density=True)\n",
    "    plt.title('Amplitude Distribution (Original vs Cleaned)')\n",
    "    plt.xlabel('Amplitude')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, ls='--', alpha=0.5)\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_hist_compare.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying amplitude distribution comparison... Close figure to continue.\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Phase picking (P and S waves) and inversion (optional)\n",
    "# ---------------------------\n",
    "# Enhanced automatic P and S phase picker (from vertical component of cleaned data)\n",
    "from scipy.signal import hilbert, butter, filtfilt, find_peaks\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def enhanced_auto_pick_phases(signal_data, time_axis, fs, min_amp=0.1, min_dist=15):\n",
    "    # Filter for P and S\n",
    "    filtered_high = bandpass_filter(signal_data, 5, 20, fs)   # high freq for P\n",
    "    filtered_low  = bandpass_filter(signal_data, 0.5, 5, fs)  # lower freq for S\n",
    "    env_high = np.abs(hilbert(filtered_high))\n",
    "    env_low  = np.abs(hilbert(filtered_low))\n",
    "    env_high /= (np.max(env_high) + 1e-9)\n",
    "    env_low  /= (np.max(env_low) + 1e-9)\n",
    "    p_peaks, _ = find_peaks(env_high, height=min_amp*1.2, distance=min_dist)\n",
    "    s_peaks, _ = find_peaks(env_low, height=min_amp, distance=min_dist*2)\n",
    "    phases = []\n",
    "    for idx in p_peaks:\n",
    "        t = time_axis[idx]\n",
    "        # Consider only early part for P\n",
    "        if t < time_axis[-1] * 0.7:\n",
    "            phases.append({'time': t, 'amplitude': env_high[idx], 'type': 'P', 'index': idx})\n",
    "    p_times = [ph['time'] for ph in phases if ph['type'] == 'P']\n",
    "    for idx in s_peaks:\n",
    "        t = time_axis[idx]\n",
    "        if t > time_axis[-1] * 0.2:  # S waves arrive later\n",
    "            # ensure not already a P within 2s (to avoid double picking same arrival)\n",
    "            if all(abs(t - tp) > 2.0 for tp in p_times):\n",
    "                phases.append({'time': t, 'amplitude': env_low[idx], 'type': 'S', 'index': idx})\n",
    "    # Sort phases by time\n",
    "    phases.sort(key=lambda x: x['time'])\n",
    "    return phases\n",
    "\n",
    "# If no seismometer channels present, skip picking\n",
    "if not do_inversion and not pick_p and not pick_s:\n",
    "    print(\"Skipping phase picking and inversion as per user choice.\")\n",
    "else:\n",
    "    # Use vertical seismometer (channel 1 in our naming) if available, otherwise first channel\n",
    "    vertical_index = 1 if num_channels > 1 else 0\n",
    "    seismogram = cleaned_data[:, vertical_index]  # use cleaned vertical channel\n",
    "    time_axis = np.arange(len(seismogram)) / sample_rate\n",
    "    print(\"Picking seismic phases (P and S) from the data...\")\n",
    "    phases = enhanced_auto_pick_phases(seismogram, time_axis, sample_rate, min_amp=0.15, min_dist=int(25))\n",
    "    # Filter phases list based on user choice\n",
    "    if not pick_p:\n",
    "        phases = [ph for ph in phases if ph['type'] != 'P']\n",
    "    if not pick_s:\n",
    "        phases = [ph for ph in phases if ph['type'] != 'S']\n",
    "    p_times = [ph['time'] for ph in phases if ph['type'] == 'P']\n",
    "    s_times = [ph['time'] for ph in phases if ph['type'] == 'S']\n",
    "    print(f\"Detected {len(p_times)} P-wave picks and {len(s_times)} S-wave picks.\")\n",
    "    # Plot the seismogram with phase pick markers\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(time_axis, seismogram, 'k-', label='Seismogram')\n",
    "    for ph in phases:\n",
    "        color = 'r' if ph['type']=='P' else 'g'\n",
    "        plt.axvline(ph['time'], color=color, linestyle='--', linewidth=1)\n",
    "        plt.text(ph['time'], np.max(seismogram)*0.8, ph['type'], color=color, fontsize=10, rotation=90, va='bottom')\n",
    "    plt.title('Seismogram with Picked Phases')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True, ls='--', alpha=0.5)\n",
    "    if save_plots:\n",
    "        fname = f\"{station_name}_{network_code}_seismogram_picks.png\"\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {fname}\")\n",
    "    print(\"Displaying seismogram with phase picks... Close figure to continue.\")\n",
    "    if HAVE_MPLCURSORS:\n",
    "        mplcursors.cursor(hover=True)\n",
    "    plt.show()\n",
    "\n",
    "    # If inversion is not requested, we stop here after picking\n",
    "    if not do_inversion:\n",
    "        print(\"Phase picking complete. Inversion was not requested. Processing finished.\")\n",
    "# ---------------------------\n",
    "# Travel-time inversion for layered structure (optional)\n",
    "# ---------------------------\n",
    "if do_inversion:\n",
    "    # Define travel-time calculation for given velocity model (P or S)\n",
    "    def calculate_travel_times(model_velocities, model_depths, offsets, wave_type='P'):\n",
    "        times = np.zeros(len(offsets))\n",
    "        # Use Vp/Vs ratio ~1.73 if S-wave\n",
    "        vel = model_velocities.copy()\n",
    "        if wave_type.upper() == 'S':\n",
    "            vel = [v/1.73 for v in vel]\n",
    "        for j, x in enumerate(offsets):\n",
    "            total_time = 0.0\n",
    "            depth_top = 0.0\n",
    "            angle = 0.0\n",
    "            for layer in range(len(model_velocities)):\n",
    "                # layer thickness\n",
    "                if layer < len(model_depths):\n",
    "                    thickness = model_depths[layer] - depth_top\n",
    "                else:\n",
    "                    thickness = 50.0  # extend last layer if needed\n",
    "                # horizontal distance that can be traveled in this layer\n",
    "                horiz = thickness * np.tan(angle) if angle != 0 else 0.0\n",
    "                if horiz + 1e-6 >= x:  # if the ray exits in this layer\n",
    "                    # remaining distance portion\n",
    "                    if angle == 0:\n",
    "                        travel = x / (vel[layer] + 1e-9)\n",
    "                    else:\n",
    "                        travel = (x - horiz) / (vel[layer] * np.cos(angle) + 1e-9)\n",
    "                    total_time += travel\n",
    "                    break\n",
    "                # full layer traverse\n",
    "                travel = thickness / (vel[layer] * np.cos(angle) + 1e-9) if angle != 0 else thickness / (vel[layer] + 1e-9)\n",
    "                total_time += travel\n",
    "                x -= horiz  # reduce remaining horizontal distance\n",
    "                depth_top = model_depths[layer] if layer < len(model_depths) else depth_top + thickness\n",
    "                # Recalculate angle for next layer using Snell's law\n",
    "                if layer < len(model_velocities) - 1:\n",
    "                    try:\n",
    "                        angle = np.arcsin((vel[layer] / vel[layer+1]) * np.sin(angle))\n",
    "                    except ValueError:\n",
    "                        angle = np.pi/2  # critical angle exceeded, horizontal propagation\n",
    "            times[j] = total_time\n",
    "        return times\n",
    "\n",
    "    # Joint inversion combining P and S travel times\n",
    "    def joint_inversion(p_obs, s_obs, offsets, init_vel, init_depth, max_iter=20, damping=0.2, tol=1e-6):\n",
    "        vel = init_vel.copy()\n",
    "        dep = init_depth.copy()\n",
    "        history = {'misfit': []}\n",
    "        # Build combined observed vector\n",
    "        obs = np.concatenate([p_obs, s_obs])\n",
    "        n_p = len(p_obs)\n",
    "        for it in range(max_iter):\n",
    "            # Predicted times\n",
    "            p_pred = calculate_travel_times(vel, dep, offsets, 'P')\n",
    "            s_pred = calculate_travel_times(vel, dep, offsets[:len(s_obs)], 'S') if len(s_obs) > 0 else np.array([])\n",
    "            pred = np.concatenate([p_pred, s_pred])\n",
    "            res = obs - pred\n",
    "            # Weighted least squares (weight S residuals higher if present)\n",
    "            weights = np.ones_like(res)\n",
    "            if len(s_obs) > 0:\n",
    "                weights[n_p:] = 1.5  # weight S\n",
    "            misfit = np.sqrt(np.mean((res * weights)**2))\n",
    "            history['misfit'].append(misfit)\n",
    "            if misfit < tol:\n",
    "                print(f\"Converged in {it} iterations, misfit={misfit:.6f}\")\n",
    "                break\n",
    "            # Simple Jacobian: finite differences approximation for each layer velocity and depth\n",
    "            n_layers = len(vel)\n",
    "            J = np.zeros((len(pred), 2*n_layers-1))  # velocity and depth (depth has one fewer)\n",
    "            delta = 0.01\n",
    "            for j in range(n_layers):\n",
    "                # Velocity partial derivative\n",
    "                vel_temp = vel.copy()\n",
    "                vel_temp[j] += delta\n",
    "                p_temp = calculate_travel_times(vel_temp, dep, offsets, 'P')\n",
    "                s_temp = calculate_travel_times(vel_temp, dep, offsets[:len(s_obs)], 'S') if len(s_obs) > 0 else np.array([])\n",
    "                pred_temp = np.concatenate([p_temp, s_temp])\n",
    "                J[:, j] = (pred_temp - pred) / delta\n",
    "                # Depth partial derivative (except last depth which is infinite)\n",
    "                if j < len(dep):\n",
    "                    dep_temp = dep.copy()\n",
    "                    dep_temp[j] += delta\n",
    "                    p_temp = calculate_travel_times(vel, dep_temp, offsets, 'P')\n",
    "                    s_temp = calculate_travel_times(vel, dep_temp, offsets[:len(s_obs)], 'S') if len(s_obs) > 0 else np.array([])\n",
    "                    pred_temp = np.concatenate([p_temp, s_temp])\n",
    "                    J[:, n_layers + j] = (pred_temp - pred) / delta\n",
    "            # Damped least squares solution\n",
    "            JTJ = J.T @ (J * weights[:, None])  # apply weights to J rows\n",
    "            JTr = J.T @ (res * weights)\n",
    "            # Solve (JTJ + damping*I) * update = J^T * residuals\n",
    "            try:\n",
    "                update = np.linalg.solve(JTJ + damping*np.eye(JTJ.shape[0]), JTr)\n",
    "            except np.linalg.LinAlgError:\n",
    "                update = np.linalg.lstsq(JTJ + damping*np.eye(JTJ.shape[0]), JTr, rcond=None)[0]\n",
    "            # Apply update (constrain to reasonable ranges)\n",
    "            for j in range(n_layers):\n",
    "                vel[j] += update[j]\n",
    "                vel[j] = max(1.0, min(vel[j], 8.5))\n",
    "            for j in range(len(dep)):\n",
    "                dep[j] += update[n_layers + j]\n",
    "                dep[j] = max(0.1, dep[j])\n",
    "            dep = np.sort(dep)  # ensure depths in ascending order\n",
    "        return vel, dep, history\n",
    "\n",
    "    # Prepare observed arrival times for inversion\n",
    "    if len(p_times) == 0:\n",
    "        print(\"No P-wave picks found; inversion cannot proceed.\")\n",
    "        do_inversion = False\n",
    "    else:\n",
    "        # Create synthetic offsets array (since actual source-receiver distances not provided, assume increasing offsets)\n",
    "        n_offsets = max(len(p_times), len(s_times), 8)  # ensure enough offsets\n",
    "        offsets = np.linspace(5, 5*n_offsets, n_offsets)  # example offset values in km\n",
    "        observed_p_times = np.array(p_times[:len(offsets)])\n",
    "        observed_s_times = np.array(s_times[:len(offsets)]) if pick_s else np.array([])\n",
    "        if len(observed_p_times) < len(offsets):\n",
    "            offsets = offsets[:len(observed_p_times)]\n",
    "        if len(observed_s_times) > len(offsets):\n",
    "            observed_s_times = observed_s_times[:len(offsets)]\n",
    "\n",
    "        # Initial model for inversion (simple crust model)\n",
    "        initial_vel = np.array([3.0, 5.5, 6.5, 7.5])  # km/s per layer\n",
    "        initial_depth = np.array([5.0, 15.0, 30.0])   # depth of interfaces (km)\n",
    "        print(\"Starting inversion with initial model:\")\n",
    "        print(f\"  Velocities = {initial_vel} km/s\")\n",
    "        print(f\"  Depths = {initial_depth} km\")\n",
    "        if len(observed_s_times) < 2:\n",
    "            # Not enough S picks for joint inversion, use P only\n",
    "            print(\"Not enough S-wave picks, performing P-wave only inversion.\")\n",
    "            observed_s_times = np.array([])\n",
    "\n",
    "        # Perform joint inversion (if S picks exist) or P-only inversion\n",
    "        final_vel, final_depth, inv_history = joint_inversion(observed_p_times, observed_s_times, offsets, initial_vel, initial_depth, max_iter=25, damping=0.2, tol=1e-4)\n",
    "        print(\"\\n=== Inversion Results ===\")\n",
    "        print(f\"Layer velocities (km/s): {np.round(final_vel, 2)}\")\n",
    "        print(f\"Layer depths (km): {np.round(final_depth, 2)}\")\n",
    "        if len(inv_history['misfit']) > 0:\n",
    "            print(f\"Final misfit: {inv_history['misfit'][-1]:.6f} s\")\n",
    "\n",
    "        # Plot observed vs calculated travel times\n",
    "        calc_p_times = calculate_travel_times(final_vel, final_depth, offsets, 'P')\n",
    "        calc_s_times = calculate_travel_times(final_vel, final_depth, offsets[:len(observed_s_times)], 'S') if len(observed_s_times)>0 else None\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(offsets, observed_p_times, 'ro', label='Observed P')\n",
    "        plt.plot(offsets, calc_p_times, 'r--', label='Calc P')\n",
    "        if len(observed_s_times) > 0:\n",
    "            off_s = offsets[:len(observed_s_times)]\n",
    "            plt.plot(off_s, observed_s_times, 'go', label='Observed S')\n",
    "            plt.plot(off_s, calc_s_times, 'g--', label='Calc S')\n",
    "        plt.xlabel('Offset [km]')\n",
    "        plt.ylabel('Travel Time [s]')\n",
    "        plt.title('Travel Time Fit')\n",
    "        plt.legend()\n",
    "        plt.grid(True, ls='--', alpha=0.5)\n",
    "        if save_plots:\n",
    "            fname = f\"{station_name}_{network_code}_travel_time_fit.png\"\n",
    "            plt.savefig(os.path.join(output_dir, fname), dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved plot: {fname}\")\n",
    "        print(\"Displaying travel-time fit plot... Close figure to finish.\")\n",
    "        if HAVE_MPLCURSORS:\n",
    "            mplcursors.cursor(hover=True)\n",
    "        plt.show()\n",
    "        print(\"Inversion complete. Processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad031fd-f3d2-41a9-a0b7-46654f7356fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa511395-ef7f-40fa-9469-7bf2e0f86111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-pygmt_env]",
   "language": "python",
   "name": "conda-env-miniforge3-pygmt_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
